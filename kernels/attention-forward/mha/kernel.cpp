// MHA Forward Attention Kernel â€” HipKittens Port
// Ported from: reference/triton/mha.py (_attn_fwd)
//
// Standard multi-head attention forward: O = softmax(Q @ K^T * scale) @ V
// Supports GQA (grouped query attention), causal masking, online softmax.
// Uses 8-wave ping-pong scheduling from the HK GQA reference kernel.
//
// Layout: Q (B, N, H, D), K (B, N, H_KV, D), V (B, N, H_KV, D), O (B, N, H, D)
// All bf16, accumulation in fp32.

#include "kittens.cuh"
#include "pyutils/pyutils.cuh"

using namespace kittens;

// Configurable via compile-time defines
#ifndef ATTN_B
constexpr int ATTN_B = 16;
#endif
#ifndef ATTN_H
constexpr int ATTN_H = 64;
#endif
#ifndef ATTN_H_KV
constexpr int ATTN_H_KV = 8;
#endif
#ifndef ATTN_N
constexpr int ATTN_N = 2048;
#endif
#ifndef ATTN_D
constexpr int ATTN_D = 128;
#endif
#ifndef IS_CAUSAL
constexpr int IS_CAUSAL = 0;
#endif

constexpr int GROUP_SIZE = ATTN_H / ATTN_H_KV;
constexpr int Q_BLOCK_SIZE = 32;
constexpr int KV_BLOCK_SIZE = 64;

#define NUM_WARPS 8
#define NUM_THREADS (kittens::WARP_THREADS * NUM_WARPS)

using _gl_QKVO = gl<bf16, -1, -1, -1, -1>;

using G = kittens::group<NUM_WARPS>;

// Tile type aliases matching the GQA reference
template<int D, typename T=bf16, typename L=row_l, typename S=rt_32x16_s>
using qo_tile = rt<T, Q_BLOCK_SIZE, D, L, S>;

template<int D, typename T=bf16, typename L=col_l, typename S=rt_16x32_s>
using qo_tile_transposed = rt<T, D, Q_BLOCK_SIZE, L, S>;

template<int D, typename T=bf16, typename L=row_l, typename S=rt_32x16_s>
using kv_tile = rt<T, KV_BLOCK_SIZE, D, L, S>;

template<int D, typename T=bf16, typename L=col_l, typename S=rt_16x32_s>
using kv_tile_transposed = rt<T, D, KV_BLOCK_SIZE, L, S>;

template<int D, typename T=float, typename L=col_l, typename S=rt_16x32_4_s>
using attn_tile = rt<T, KV_BLOCK_SIZE, Q_BLOCK_SIZE, L, S>;

// exp2 helper on base tiles
template<typename T, ducks::rt_layout::all layout, ducks::rt_shape::all shape>
__device__ inline void exp2(rt_base<T, layout, shape> &dst, const rt_base<T, layout, shape> &src) {
    #pragma unroll
    for(int k = 0; k < dst.packed_per_thread; k++) {
        dst.data[k] = base_ops::exp2::op(src.data[k]);
    }
}

// Rescale threshold for lazy rescaling optimization
constexpr float RESCALE_THRESHOLD = 8.0f;

template<typename RV>
__device__ __forceinline__ int rv_all_below(const RV& prev, const RV& cur, float T) {
    int ok = 1;
    #pragma unroll
    for (int o = 0; o < RV::outer_dim; ++o) {
        #pragma unroll
        for (int i = 0; i < RV::inner_dim; ++i) {
            ok &= (float(cur.data[o][i]) - float(prev.data[o][i]) <= T);
        }
    }
    return ok;
}

__device__ __forceinline__ int wave_all_ok(int lane_ok) {
    return __all(lane_ok);
}

template<int D>
struct mha_globals {
    _gl_QKVO Qg, Kg, Vg, Og;
    gl<float, -1, -1, -1, -1> L_vec;
    hipStream_t stream;
    dim3 grid() {
        return dim3(ATTN_H,
                    ((ATTN_N / Q_BLOCK_SIZE + NUM_WARPS - 1) / NUM_WARPS),
                    ATTN_B);
    }
    dim3 block() { return dim3(NUM_THREADS); }
    size_t dynamic_shared_memory() { return MAX_SHARED_MEMORY; }
};

template<int D>
__launch_bounds__(NUM_THREADS, 2)
__global__ void mha_fwd_kernel(const mha_globals<D> g) {
    extern __shared__ alignment_dummy __shm[];
    shared_allocator al((int*)&__shm[0]);
    st_bf<KV_BLOCK_SIZE, ATTN_D, st_32x32_s> (&k_smem)[2] = al.allocate<st_bf<KV_BLOCK_SIZE, ATTN_D, st_32x32_s>, 2>();
    st_bf<KV_BLOCK_SIZE, ATTN_D, st_8x32_s> (&v_smem)[2] = al.allocate<st_bf<KV_BLOCK_SIZE, ATTN_D, st_8x32_s>, 2>();

    // Index calculations with GQA head mapping
    const int head_idx = (blockIdx.x % ATTN_H_KV) * GROUP_SIZE + (blockIdx.x / ATTN_H_KV);
    const int batch_idx = blockIdx.z;
    const int head_idx_kv = head_idx / GROUP_SIZE;
    const int block_tile_idx = blockIdx.y;
    const int tile_idx = block_tile_idx * NUM_WARPS + warpid();

    // Causal: early exit if this tile is fully masked
    if constexpr (IS_CAUSAL) {
        const int q_start = tile_idx * Q_BLOCK_SIZE;
        if (q_start >= ATTN_N) return;
    }

    // Buffer resource setup for async loads
    const bf16* k_base = (bf16*)&g.Kg[{batch_idx, 0, head_idx_kv, 0}];
    const bf16* v_base = (bf16*)&g.Vg[{batch_idx, 0, head_idx_kv, 0}];
    const int k_row_stride = g.Kg.template stride<1>() * sizeof(bf16);
    const int v_row_stride = g.Vg.template stride<1>() * sizeof(bf16);
    i32x4 k_srsrc_base = make_srsrc(k_base, k_row_stride * ATTN_N, k_row_stride);
    i32x4 v_srsrc_base = make_srsrc(v_base, v_row_stride * ATTN_N, v_row_stride);

    const int wid = warpid() % NUM_WARPS;
    constexpr int elem_per_warp = (16 / sizeof(bf16)) * kittens::WARP_THREADS;
    uint32_t k_lds_base_0 = __builtin_amdgcn_readfirstlane(static_cast<uint32_t>(
        reinterpret_cast<uintptr_t>(&k_smem[0].data[0]) + wid * elem_per_warp * sizeof(bf16)));
    uint32_t v_lds_base_0 = __builtin_amdgcn_readfirstlane(static_cast<uint32_t>(
        reinterpret_cast<uintptr_t>(&v_smem[0].data[0]) + wid * elem_per_warp * sizeof(bf16)));
    uint32_t k_lds_base_1 = __builtin_amdgcn_readfirstlane(static_cast<uint32_t>(
        reinterpret_cast<uintptr_t>(&k_smem[1].data[0]) + wid * elem_per_warp * sizeof(bf16)));
    uint32_t v_lds_base_1 = __builtin_amdgcn_readfirstlane(static_cast<uint32_t>(
        reinterpret_cast<uintptr_t>(&v_smem[1].data[0]) + wid * elem_per_warp * sizeof(bf16)));

    // Determine number of KV tiles (with causal limit)
    int num_tiles = ATTN_N / KV_BLOCK_SIZE;
    if constexpr (IS_CAUSAL) {
        const int q_end = (tile_idx + 1) * Q_BLOCK_SIZE;
        int causal_limit = (q_end + KV_BLOCK_SIZE - 1) / KV_BLOCK_SIZE;
        num_tiles = min(num_tiles, causal_limit);
    }

    constexpr float TEMPERATURE_SCALE = (D == 128)
        ? 0.08838834764f * 1.44269504089f
        : 0.125f * 1.44269504089f;

    // Register tiles
    qo_tile<D, bf16> q_reg;
    qo_tile_transposed<D, bf16> q_reg_transposed;
    kv_tile<D, bf16> k_reg;
    kv_tile_transposed<D, bf16> k_reg_transposed;
    kv_tile<D, bf16, col_l, rt_16x32_4_s> v_reg;
    qo_tile_transposed<D, float, col_l, rt_32x32_s> o_reg;
    attn_tile<D, float, col_l, rt_32x32_s> att_block[2];
    attn_tile<D, bf16, col_l, rt_32x32_s> att_block_bf16;
    attn_tile<D, bf16, col_l, rt_16x32_4_s> att_block_bf16_in;
    typename attn_tile<D, float, col_l, rt_32x32_s>::row_vec max_vec, norm_vec, max_vec_prev, scale_vec;

    kittens::zero(o_reg);
    kittens::zero(norm_vec);
    kittens::ones(scale_vec);

    // Swizzled offset precomputation
    using T = typename st_bf<KV_BLOCK_SIZE, ATTN_D, st_32x32_s>::dtype;
    constexpr int bytes_per_thread = st_32x32_s::template bytes_per_thread<T>();
    constexpr int bytes_per_memcpy = bytes_per_thread * NUM_THREADS;
    constexpr int memcpy_per_tile = KV_BLOCK_SIZE * ATTN_D * sizeof(T) / bytes_per_memcpy;

    uint32_t swizzled_offsets_V[memcpy_per_tile];
    uint32_t swizzled_offsets_K[memcpy_per_tile];
    G::prefill_swizzled_offsets<1, false>(k_smem[0], g.Kg, swizzled_offsets_K);
    G::prefill_swizzled_offsets<1, false>(v_smem[0], g.Vg, swizzled_offsets_V);

    // Load Q and first K tile
    G::load<1, false>(k_smem[0], g.Kg, {batch_idx, 0, head_idx_kv, 0}, swizzled_offsets_K, k_srsrc_base, k_base, k_lds_base_0);
    __builtin_amdgcn_s_waitcnt(0);
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();

    qo_tile<D, float> q_reg_fl;
    load<1, qo_tile<D, float>, _gl_QKVO>(q_reg_fl, g.Qg, {batch_idx, tile_idx, head_idx, 0});
    mul(q_reg_fl, q_reg_fl, TEMPERATURE_SCALE);
    copy(q_reg, q_reg_fl);
    transpose(q_reg_transposed, q_reg);

    // Load K1 and V0
    if (num_tiles > 1) {
        G::load<1, false>(k_smem[1], g.Kg, {batch_idx, 1, head_idx_kv, 0}, swizzled_offsets_K, k_srsrc_base, k_base, k_lds_base_1);
    }
    G::load<1, false>(v_smem[0], g.Vg, {batch_idx, 0, head_idx_kv, 0}, swizzled_offsets_V, v_srsrc_base, v_base, v_lds_base_0);
    load(k_reg, k_smem[0]);
    __builtin_amdgcn_sched_barrier(0);
    asm volatile("s_waitcnt lgkmcnt(0)");
    asm volatile("s_waitcnt vmcnt(2)");
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();

    // QK0
    kittens::zero(att_block[0]);
    transpose(k_reg_transposed, k_reg);
    mma_AtB(att_block[0], k_reg_transposed, q_reg_transposed, att_block[0]);

    // Causal masking for first KV block
    if constexpr (IS_CAUSAL) {
        // Apply causal mask: mask positions where k_pos > q_pos
        // q positions: tile_idx * Q_BLOCK_SIZE + [0..Q_BLOCK_SIZE)
        // k positions for block 0: [0..KV_BLOCK_SIZE)
        // Since block 0 starts at k=0, only need mask if q_start < KV_BLOCK_SIZE
    }

    // Partial softmax of QK0
    col_max(max_vec, att_block[0]);
    copy(max_vec_prev, max_vec);
    sub_col(att_block[0], att_block[0], max_vec);
    exp2(att_block[0].tiles[0][0], att_block[0].tiles[0][0]);
    __builtin_amdgcn_sched_barrier(0);

    const int stagger = warpid() / 4;
    if (stagger) {
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
    }

    __builtin_amdgcn_sched_barrier(0);
    if (num_tiles > 1) {
        load(k_reg, k_smem[1]);
    }
    if (num_tiles > 2) {
        G::load<1, false>(k_smem[0], g.Kg, {batch_idx, 2, head_idx_kv, 0}, swizzled_offsets_K, k_srsrc_base, k_base, k_lds_base_0);
    }
    G::load<1, false>(v_smem[1], g.Vg, {batch_idx, 1, head_idx_kv, 0}, swizzled_offsets_V, v_srsrc_base, v_base, v_lds_base_1);
    asm volatile("s_waitcnt lgkmcnt(0)");
    asm volatile("s_waitcnt vmcnt(4)");
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();

    int pending_scale = 0;

    // Main loop: 8-wave ping-pong pattern (processes 2 KV tiles per iteration)
    #pragma unroll 2
    for (int j = 3; j < num_tiles - 1; j += 2) {
        // Phase A: Process odd KV tile
        kittens::zero(att_block[1]);
        transpose(k_reg_transposed, k_reg);
        if (pending_scale) {
            mul(norm_vec, norm_vec, scale_vec);
        }
        mma_AtB(att_block[1], k_reg_transposed, q_reg_transposed, att_block[1]);
        copy(subtile_inplace<32>(att_block_bf16, 0), subtile_inplace<32>(att_block[0], 0));
        exp2(att_block[0].tiles[1][0], att_block[0].tiles[1][0]);
        col_sum(norm_vec, att_block[0], norm_vec);
        copy(subtile_inplace<32>(att_block_bf16, 1), subtile_inplace<32>(att_block[0], 1));
        att_block_bf16_in = *reinterpret_cast<attn_tile<D, bf16, col_l, rt_16x32_4_s>*>(&att_block_bf16);
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
        __builtin_amdgcn_sched_barrier(0);

        G::load<1, false>(k_smem[1], g.Kg, {batch_idx, j, head_idx_kv, 0}, swizzled_offsets_K, k_srsrc_base, k_base, k_lds_base_1);
        load(v_reg, v_smem[0]);
        asm volatile("s_waitcnt lgkmcnt(0)");
        asm volatile("s_waitcnt vmcnt(4)");
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
        __builtin_amdgcn_sched_barrier(0);

        // AV multiply
        __builtin_amdgcn_s_setprio(1);
        mma_AtB(o_reg, subtile_inplace<16>(v_reg, 0), subtile_inplace<16>(att_block_bf16_in, 0), o_reg);
        col_max(max_vec, att_block[1], max_vec_prev);
        int lane_ok = rv_all_below(max_vec_prev, max_vec, RESCALE_THRESHOLD);
        int all_ok = wave_all_ok(lane_ok);
        if (__builtin_expect(all_ok, 1)) {
            copy(max_vec, max_vec_prev);
            pending_scale = 0;
        } else {
            sub(scale_vec, max_vec_prev, max_vec);
            exp2(scale_vec, scale_vec);
            mul_col(o_reg, o_reg, scale_vec);
            copy(max_vec_prev, max_vec);
            pending_scale = 1;
        }
        mma_AtB(o_reg, subtile_inplace<16>(v_reg, 1), subtile_inplace<16>(att_block_bf16_in, 1), o_reg);
        mma_AtB(o_reg, subtile_inplace<16>(v_reg, 2), subtile_inplace<16>(att_block_bf16_in, 2), o_reg);
        mma_AtB(o_reg, subtile_inplace<16>(v_reg, 3), subtile_inplace<16>(att_block_bf16_in, 3), o_reg);
        sub_col(att_block[1], att_block[1], max_vec);
        exp2(att_block[1].tiles[0][0], att_block[1].tiles[0][0]);
        __builtin_amdgcn_s_setprio(0);
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
        __builtin_amdgcn_sched_barrier(0);

        G::load<1, false>(v_smem[0], g.Vg, {batch_idx, j - 1, head_idx_kv, 0}, swizzled_offsets_V, v_srsrc_base, v_base, v_lds_base_0);
        load(k_reg, k_smem[0]);
        asm volatile("s_waitcnt lgkmcnt(0)");
        asm volatile("s_waitcnt vmcnt(4)");
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
        __builtin_amdgcn_sched_barrier(0);

        // Phase B: Process even KV tile
        kittens::zero(att_block[0]);
        transpose(k_reg_transposed, k_reg);
        if (pending_scale) {
            mul(norm_vec, norm_vec, scale_vec);
        }
        mma_AtB(att_block[0], k_reg_transposed, q_reg_transposed, att_block[0]);
        exp2(att_block[1].tiles[1][0], att_block[1].tiles[1][0]);
        col_sum(norm_vec, att_block[1], norm_vec);
        copy(att_block_bf16, att_block[1]);
        att_block_bf16_in = *reinterpret_cast<attn_tile<D, bf16, col_l, rt_16x32_4_s>*>(&att_block_bf16);
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
        __builtin_amdgcn_sched_barrier(0);

        G::load<1, false>(k_smem[0], g.Kg, {batch_idx, j + 1, head_idx_kv, 0}, swizzled_offsets_K, k_srsrc_base, k_base, k_lds_base_0);
        load(v_reg, v_smem[1]);
        asm volatile("s_waitcnt lgkmcnt(0)");
        asm volatile("s_waitcnt vmcnt(4)");
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
        __builtin_amdgcn_sched_barrier(0);

        __builtin_amdgcn_s_setprio(1);
        mma_AtB(o_reg, subtile_inplace<16>(v_reg, 0), subtile_inplace<16>(att_block_bf16_in, 0), o_reg);
        col_max(max_vec, att_block[0], max_vec_prev);
        lane_ok = rv_all_below(max_vec_prev, max_vec, RESCALE_THRESHOLD);
        all_ok = wave_all_ok(lane_ok);
        if (__builtin_expect(all_ok, 1)) {
            copy(max_vec, max_vec_prev);
            pending_scale = 0;
        } else {
            sub(scale_vec, max_vec_prev, max_vec);
            exp2(scale_vec, scale_vec);
            mul_col(o_reg, o_reg, scale_vec);
            copy(max_vec_prev, max_vec);
            pending_scale = 1;
        }
        mma_AtB(o_reg, subtile_inplace<16>(v_reg, 1), subtile_inplace<16>(att_block_bf16_in, 1), o_reg);
        mma_AtB(o_reg, subtile_inplace<16>(v_reg, 2), subtile_inplace<16>(att_block_bf16_in, 2), o_reg);
        mma_AtB(o_reg, subtile_inplace<16>(v_reg, 3), subtile_inplace<16>(att_block_bf16_in, 3), o_reg);
        sub_col(att_block[0], att_block[0], max_vec);
        exp2(att_block[0].tiles[0][0], att_block[0].tiles[0][0]);
        __builtin_amdgcn_s_setprio(0);
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
        __builtin_amdgcn_sched_barrier(0);

        G::load<1, false>(v_smem[1], g.Vg, {batch_idx, j, head_idx_kv, 0}, swizzled_offsets_V, v_srsrc_base, v_base, v_lds_base_1);
        load(k_reg, k_smem[1]);
        asm volatile("s_waitcnt lgkmcnt(0)");
        asm volatile("s_waitcnt vmcnt(4)");
        __builtin_amdgcn_sched_barrier(0);
        __builtin_amdgcn_s_barrier();
        __builtin_amdgcn_sched_barrier(0);
    }

    // Epilogue: process last 2-3 KV tiles
    // Process second-to-last pair
    kittens::zero(att_block[1]);
    transpose(k_reg_transposed, k_reg);
    mma_AtB(att_block[1], k_reg_transposed, q_reg_transposed, att_block[1]);
    exp2(att_block[0].tiles[1][0], att_block[0].tiles[1][0]);
    mul(norm_vec, norm_vec, scale_vec);
    col_sum(norm_vec, att_block[0], norm_vec);
    copy(att_block_bf16, att_block[0]);
    att_block_bf16_in = *reinterpret_cast<attn_tile<D, bf16, col_l, rt_16x32_4_s>*>(&att_block_bf16);
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    G::load<1, false>(k_smem[1], g.Kg, {batch_idx, num_tiles - 1, head_idx_kv, 0}, swizzled_offsets_K, k_srsrc_base, k_base, k_lds_base_1);
    load(v_reg, v_smem[0]);
    asm volatile("s_waitcnt lgkmcnt(0)");
    asm volatile("s_waitcnt vmcnt(4)");
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    __builtin_amdgcn_s_setprio(1);
    mma_AtB(o_reg, v_reg, att_block_bf16_in, o_reg);
    col_max(max_vec, att_block[1], max_vec_prev);
    sub(scale_vec, max_vec_prev, max_vec);
    copy(max_vec_prev, max_vec);
    exp2(scale_vec, scale_vec);
    sub_col(att_block[1], att_block[1], max_vec);
    exp2(att_block[1].tiles[0][0], att_block[1].tiles[0][0]);
    mul_col(o_reg, o_reg, scale_vec);
    __builtin_amdgcn_s_setprio(0);
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    G::load<1, false>(v_smem[0], g.Vg, {batch_idx, num_tiles - 2, head_idx_kv, 0}, swizzled_offsets_V, v_srsrc_base, v_base, v_lds_base_0);
    load(k_reg, k_smem[0]);
    asm volatile("s_waitcnt lgkmcnt(0)");
    asm volatile("s_waitcnt vmcnt(4)");
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    kittens::zero(att_block[0]);
    transpose(k_reg_transposed, k_reg);
    mma_AtB(att_block[0], k_reg_transposed, q_reg_transposed, att_block[0]);
    exp2(att_block[1].tiles[1][0], att_block[1].tiles[1][0]);
    mul(norm_vec, norm_vec, scale_vec);
    col_sum(norm_vec, att_block[1], norm_vec);
    copy(att_block_bf16, att_block[1]);
    att_block_bf16_in = *reinterpret_cast<attn_tile<D, bf16, col_l, rt_16x32_4_s>*>(&att_block_bf16);
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    load(v_reg, v_smem[1]);
    asm volatile("s_waitcnt lgkmcnt(0)");
    asm volatile("s_waitcnt vmcnt(2)");
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    __builtin_amdgcn_s_setprio(1);
    mma_AtB(o_reg, v_reg, att_block_bf16_in, o_reg);
    col_max(max_vec, att_block[0], max_vec_prev);
    sub(scale_vec, max_vec_prev, max_vec);
    copy(max_vec_prev, max_vec);
    exp2(scale_vec, scale_vec);
    sub_col(att_block[0], att_block[0], max_vec);
    exp2(att_block[0].tiles[0][0], att_block[0].tiles[0][0]);
    mul_col(o_reg, o_reg, scale_vec);
    __builtin_amdgcn_s_setprio(0);
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    G::load<1, false>(v_smem[1], g.Vg, {batch_idx, num_tiles - 1, head_idx_kv, 0}, swizzled_offsets_V, v_srsrc_base, v_base, v_lds_base_1);
    load(k_reg, k_smem[1]);
    asm volatile("s_waitcnt lgkmcnt(0)");
    asm volatile("s_waitcnt vmcnt(2)");
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    // Last KV tile
    kittens::zero(att_block[1]);
    transpose(k_reg_transposed, k_reg);
    mma_AtB(att_block[1], k_reg_transposed, q_reg_transposed, att_block[1]);
    exp2(att_block[0].tiles[1][0], att_block[0].tiles[1][0]);
    mul(norm_vec, norm_vec, scale_vec);
    col_sum(norm_vec, att_block[0], norm_vec);
    copy(att_block_bf16, att_block[0]);
    att_block_bf16_in = *reinterpret_cast<attn_tile<D, bf16, col_l, rt_16x32_4_s>*>(&att_block_bf16);
    __builtin_amdgcn_sched_barrier(0);

    load(v_reg, v_smem[0]);
    asm volatile("s_waitcnt lgkmcnt(0)");
    asm volatile("s_waitcnt vmcnt(0)");
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    mma_AtB(o_reg, v_reg, att_block_bf16_in, o_reg);
    col_max(max_vec, att_block[1], max_vec_prev);
    sub(scale_vec, max_vec_prev, max_vec);
    copy(max_vec_prev, max_vec);
    exp2(scale_vec, scale_vec);
    sub_col(att_block[1], att_block[1], max_vec);
    exp2(att_block[1].tiles[0][0], att_block[1].tiles[0][0]);
    exp2(att_block[1].tiles[1][0], att_block[1].tiles[1][0]);
    mul(norm_vec, norm_vec, scale_vec);
    col_sum(norm_vec, att_block[1], norm_vec);
    copy(att_block_bf16, att_block[1]);
    att_block_bf16_in = *reinterpret_cast<attn_tile<D, bf16, col_l, rt_16x32_4_s>*>(&att_block_bf16);
    __builtin_amdgcn_sched_barrier(0);
    mul_col(o_reg, o_reg, scale_vec);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    load(v_reg, v_smem[1]);
    asm volatile("s_waitcnt lgkmcnt(0)");
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    // Final AV and normalize
    mma_AtB(o_reg, v_reg, att_block_bf16_in, o_reg);
    div_col(o_reg, o_reg, norm_vec);
    __builtin_amdgcn_sched_barrier(0);
    __builtin_amdgcn_s_barrier();
    __builtin_amdgcn_sched_barrier(0);

    if (!stagger) {
        __builtin_amdgcn_s_barrier();
    }

    // Store output
    qo_tile<D, float, row_l, rt_32x32_s> o_reg_transposed;
    transpose(o_reg_transposed, o_reg);
    store<1>(g.Og, o_reg_transposed, {batch_idx, tile_idx, head_idx, 0});

    // Store LSE = ln(2) * max + ln(norm)
    mul(max_vec, max_vec, 0.69314718056f);
    log(norm_vec, norm_vec);
    add(norm_vec, norm_vec, max_vec);
    store(g.L_vec, norm_vec, {batch_idx, head_idx, 0, tile_idx});
}

template<int D>
void dispatch_mha(mha_globals<D> g) {
    unsigned long mem_size = g.dynamic_shared_memory();
    hipFuncSetAttribute((void*)mha_fwd_kernel<D>,
        hipFuncAttributeMaxDynamicSharedMemorySize, mem_size);
    mha_fwd_kernel<D><<<g.grid(), g.block(), mem_size, g.stream>>>(g);
}

PYBIND11_MODULE(tk_mha_fwd, m) {
    m.doc() = "HipKittens MHA Forward Attention kernel";
    py::bind_function<dispatch_mha<ATTN_D>>(m, "dispatch",
        &mha_globals<ATTN_D>::Qg,
        &mha_globals<ATTN_D>::Kg,
        &mha_globals<ATTN_D>::Vg,
        &mha_globals<ATTN_D>::Og,
        &mha_globals<ATTN_D>::L_vec
    );
}
